---
title: "Logistic Regression Models"
author: "Michael Clark"
output: 
  html_document: 
    css: ../style_for_miles_and_miles_so_much_style_that_its_wasted.css
    highlight: tango
    theme: sandstone
    toc: yes
    toc_float: yes
    smart: yes
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(message=F, fig.align='center', cache = FALSE, comment='', warning=F, R.options=list(width=120))
library(ggplot2); library(dplyr); library(lazerhawk); library(tidyr)
```

# Introduction

Clarification for the uninitiated.  Many in applied disciplines are exposed to standard linear regression and little more.  Or they might have a cursory glance at logistic regression, where it is presented in a manner such that one may come away feeling it is a wholly different type of analysis, even though some versions are a generalized linear model just as standard regression is.  Furthermore, there is a bit of a nomenclature issue, that again might make some think they are dealing with wholly different models, when in fact they are dealing with extensions of simpler ones they've been exposed to. This document hopes to provide a bit of clarity in this realm.

# Two categories

## Data

The most common use of logistic regression is the case where the target variable we wish to understand is a binary variable, e.g. yes/no, buy/sell, dead/alive etc.  Aside from the standard linear model, this is probably the most common statistical modeling situation.

### Binomial Distribution

As the outcome is binary, we can potentially understand the data generating process as a <span class="emph">binomial</span> distribution. The binomial is typically one of the first probability distributions introduced in a statistics class.  The classic example involves a coin flipping situation.  

Going with that, let's say we flip the coin 10 times. The chance it comes up heads or tails is 50/50, i.e. the probability = .5. If we do this once, who knows how many times heads will come up.

```{r binom1trial}
sample(c('heads','tails'), 10, prob=c(.5, .5), replace=T)
```


However, if we repeat it over and over, we can see how often we might expect each possible outcome of 10 coin flips. The binomial has a parameter $\pi$, the probability that the event in question occurs. We set the size, or number of 'trials', accordingly.

```{r binom, echo=1}
output = rbinom(5000, size=10, prob=.5)

qplot(factor(output), geom='bar') + xlab('') + theme_trueMinimal()
```

Since the probability is .5, we would expect an outcome of 5 heads out of 10 more than other outcomes.

Now what if we have a situation where there is only one coin flip? In this case the size or number of trials is 1, and the distribution would look like the following.

```{r binom_size1, echo=1}
output = rbinom(1000, size=1, prob=.5)

qplot(factor(output, labels=c('heads','tails')), geom='bar', width=.5) + xlab('') + theme_trueMinimal()
```

Now we have come to the most common form of logistic regression. The target variable we wish to understand is binary, and the number of times in which it is observed is once, i.e. once per individual, tweet, firm, country, or whatever our data regards. 


## Logistic Regression Model

We'll start by writing up the data formally. For reference I'll start with the standard linear model for regression just to get our bearings.  We depict it as follows:

$$ \mu = b_0 + b_1*x_1 + b_2*x_2 \dots b_p*x_p $$
$$ \mu = X\beta $$
$$ y \sim \mathcal{N}(\mu, \sigma^2)$$

In the above $\mu$ is the linear predictor, the weighted combination of $p$ covariates $x$, written two ways, one explicit and one using matrix notation, where $X$ is the model matrix and $\beta$ the vector of coefficients.  The former is for those who are not familiar with matrix notation, but will not be used again, as the latter can just be considered shorthand[^subscript].  The coefficients we wish to estimate are $\beta$, and for the normal distribution we also need to estimate the variance $\sigma^2$. 

For binary target variables we do not assume the data generating process is a normal distribution, but instead we often consider a binomial as above.  The Bernoulli distribution is a special case of the binomial for the case of modeling a binary outcome (where size=1), and might be more optimal to use for some modeling approaches (e.g. Stan).  With logistic regression, the linear predictor is the <span class="emph">logit</span>, or log of the probability of the specific label of interest over the 1 minus that probability. Note that which label we refer to is arbitrary, e.g. whether you want the probability to regard a 'yes' outcome or 'no' is entirely up to you, though generally it's a good idea to be more sensical than SPSS is. 

The logit is the name for the (natural) log of the odds, $\pi/(1-\pi)$, i.e. the ratio of the probability of the event of interest, $\pi$, to the probability of its non-occurrence. It theoretically ranges from $-\infty$ to $\infty$ and is centered at zero, which is akin to a probability of .5.  The logit is assumed to be some function of the covariates.

$$ \textrm{Logit}\: \vcenter{:}\mathord{=} \:\ln(\frac{\pi}{1-\pi}) $$
$$ \textrm{Logit} = X\beta $$

The transformation function, or <span class="emph">link function</span>, of interest is the <span class="emph">logistic</span> link, hence the name logistic regression. Probabilities are inherently nonlinear, e.g. the change from .05 to .10 is a doubling of the probability, while that from .50 to .55 is only a 10% increase. To engage in a linear model, the logistic link transforms the probability response to the logit.  Converting back to the probability scale requires the inverse logistic function, which might be depicted in different ways.

<a id="binpred"></a> 
$$ \pi = \textrm{Logit}^{-1}$$
$$ \pi = \frac{1}{1+e^{-XB}}, \textrm{or}$$
$$ \pi = \frac{e^{XB}}{1+e^{XB}}$$

And finally, given the probability, we can get the likelihood of the the response given that probability.

$$ y \sim \mathrm{Bin}(\pi, \mathrm{size}=1), \textrm{ or} $$
$$ y \sim \mathrm{Bern}(\pi) $$



To make this clearer, let's convert a value presumed to be on the logit scale to a probability.

```{r logittoprob}
plogis(0)          # convert to probability
log(.5/(1-.5))     # logit
plogis(-1)         # base R approach
plogis(1)          # 1 minus plogis(-1)
1/(1+exp(-1))
exp(1)/(1+exp(1))
```


I will say that calling a model by its link function is odd to me for several reasons: there are several link functions one might use, the logistic link function is used for other models (e.g. ordinal, neural nets), just calling it logistic regression doesn't tell you how many categories are present, and we don't do this with other models. For example, a very common alternative is the <span class="emph">probit</span> link function, which uses the cumulative normal distribution (e.g. <span class="func">qnorm</span> in R, instead of <span class="func">plogis</span>) to convert the probability scale to the logit. *Anything* that converts the linear predictor to $(0,1)$ will technically work. However, calling it a 'link' model does not change what one thinks about the underlying response distribution in this setting.

$$ y \sim \mathrm{Bern}(\pi) \quad \scriptsize{\textrm{(a probit model, same as the logistic model)}} $$ 

An example of different link functions is shown in the next figure.  There are sometimes reasons to prefer one over another, e.g. considering what one thinks about the tails of the distribution, and some can be seen as special cases of others (e.g. complementary log-log is a special case of generalized extreme value). The choice becomes more important in multinomial and ordinal models.

```{r links, echo=FALSE}
LP = rnorm(2500)
Logit = plogis(LP)
Probit = pnorm(LP)
Probit_t = extraDistr::pnst(LP, df=1)
Extreme = exp(-exp(-LP))  # Generalized EV, with shape parameter = 0 (Gumbel) = cloglog
# qplot(LP, exp(-LP)*exp(-exp(-LP)), geom='line') # density function
# Cloglog = 1-exp(-exp(LP))
dlinks = tidyr::gather(data_frame(`Linear Predictor`=LP,Logit,Probit, `T`=Probit_t, 
                                  `Extreme Value`=Extreme),  #, Cloglog
                       key=Link, value=Probability, -`Linear Predictor`)
ggplot(aes(x=`Linear Predictor`, y=Probability), data=dlinks) +
  geom_line(aes(color=Link)) +
  theme_trueMinimal()
```


## Example

We'll use the example from the [UCLA ATS website](http://www.ats.ucla.edu/stat/r/dae/logit.htm), in case one wants a bit more detail or see it with languages other than R. The hypothetical data regards graduate school admission, and we have undergraduate gpa, gre scores, and school prestige, a variable with values 1 through 4 where institutions with a rank of 1 have the highest prestige, and 4 the lowest.


```{r read_admit_data, echo=1}
admission <- read.csv("data/admit.csv")
DT::datatable(admission, width=400, style='bootstrap', rownames=FALSE, options=list(pageLength=5, dom='t'))
```

In this case our binary target is whether a prospective candidate is admitted or not (1=admitted, 0 not), and we wish to predict it with the other variables.

```{r logreg}
mod = glm(admit ~ gre + gpa + rank, data=admission, family=binomial)
summary(mod)
```


## Interpretation

The effects seen are in the directions expected, i.e. higher gre and gpa and more prestige suggests more likely admittance (recall lower prestige rank score means higher prestige).

### Odds ratios

The coefficients tell us what happens on the logit scale, which is perhaps not that interpretable, except in the usual regression sense. If I move up one value on gpa, the logit increases .77.  People typically convert the coefficients to <span class="emph">odds ratios</span>, which we obtain by exponentiating the coefficients.

```{r odds_ratios}
exp(coef(mod))
```

Now if I move up one on gpa, the odds of being admitted increase by a factor of `r round(exp(coef(mod))[3], 1)`, i.e. more than double. If my prestige rank increases by one (i.e. less prestige), the odds decrease by a `r 100*(1-round(exp(coef(mod))[4], 2))`%.


Unless you do a lot of gambling, odds ratios probably aren't all that interpretable either. One can get the estimated probabilities at key values of the covariates, which are easier to understand.

```{r logreg_predict}
prediction_data = data.frame(gre=mean(admission$gre), gpa=mean(admission$gpa), rank=c(1,4))
predict(mod, newdata=prediction_data)                    # logit scale
predict(mod, newdata=prediction_data, type='response')   # probability scale
```

Thus even at average gre and gpa, one has a ~50% of being admitted to graduate school if they went to a highly prestigious school versus one that is not.

To make things perfectly clear, let's do it by hand.

```{r prediction}
coefs = coef(mod)
prediction = coefs[1] + coefs[2]*mean(admission$gre)  + coefs[3]*mean(admission$gpa) + coefs[4]*c(1,4)
prediction
plogis(prediction)
```

The nonlinear nature of the model becomes clear if we visualize the relationships of covariates on the logit versus probability scales. We'll look at the gpa effect.

```{r plot_prediction, echo=1:4, fig.width=4}
prediction_data = data.frame(gre = mean(admission$gre),
                              gpa = rep(seq(from = 0, to = 4, length.out = 100), 4), rank = 1)

preds_logit = predict(mod, newdata = prediction_data, type="link")
preds_prob = predict(mod, newdata = prediction_data, type="response")
# 
# ggplot(aes(x=gpa, y=preds_logit), data=prediction_data) +
#   geom_line(color='#ff5503') + 
#   ylim(c(-3,3)) +
#   theme_trueMinimal()
# ggplot(aes(x=gpa, y=preds_prob), data=prediction_data) +
#   geom_line(color='#ff5503') + 
#     ylim(c(0,1)) +
#   theme_trueMinimal()

library(plotly)
y2layout <- list(
  # tickfont = list(color = "red"),
  overlaying = "y",
  side = "right",
  range = c(0,1),
  dtick = 1/4,
  showgrid=F
)

y1layout <- list(
  range = c(-3,3),
  dtick = 1
)
plotdata = data.frame(prediction_data, preds_logit, preds_prob) %>% arrange(gpa)
plot_ly(x=gpa, y=preds_logit, data=plotdata, yaxis='y1', name='Logit', width=800) %>% 
  add_trace(x=gpa, y=preds_prob, yaxis = "y2", name='Probability Scale') %>%
  layout(yaxis2 = y2layout, yaxis=y1layout) %>% 
  theme_plotly()
```


## Summary of Standard Logistic Regression

So there you have it.  The standard logistic regression is the simplest setting for a categorical outcome, one in which there are two possibilities, and only one of which can occur. It is a special case of multinomial, ordinal, and conditional logistic regression, and so can serve as a starting point for moving toward those models.


## Extensions
### Counts

As noted, the binomial distribution refers to situations in which an event occurs x number of times out of some number of trials/observations.  In the binary logistic regression model, the number of trials is 1, but it certainly doesn't have to be.  If it is more than 1 we can then model the proportion, and at least in R the glm function is still used with family = binomial just as before.  We just specify the target variable differently, in terms of the number of times the vs. the number of times it did not.

```{r binomialreg, eval=F}
glm(cbind(occurrences, non-occurrences) ~ x + z, data=mydata, family=binomial)
```


#### Link with Poisson

If the occurrences are rare and/or the total number of trials is unknown, then the model is equivalent to Poisson regression.  For more on this see [link]() 

### Conditional Logistic

Situations arise when there are alternative specific covariates, such that the value a covariate takes can be different for the two outcomes.  This is the first step toward *discrete choice* models (a.k.a. McFadden choice model), in which there are often more than two choices (as we will see with multinomial models), and values vary with choice.  The key idea is that we have strata or groups which contain both positive and negative target values.  However, the model is similar.

$$ \textrm{Logit} \propto X\beta $$

The odds of the event are *proportional to* the linear combination of the covariates.  This generalizes the previous logistic regression model depicted, as it can be seen as a special case. 


#### Example

The following example regards infertility after spontaneous and induced abortion. It is a matched case-control study such that we have exactly one 'case', i.e. person with infertility, per two control observations[^stratum]. We will model this with covariates that regard whether or not they previously had a spontaneous or induced abortion.

```{r condlogreg}
infert = infert %>%
  mutate(spontaneous = factor(spontaneous >0, labels=c('No','Yes')),
         induced = factor(induced>0, labels=c('No','Yes')))
library(survival)
# ?infert
mod_logreg = glm(case ~ spontaneous + induced, data = infert, family = binomial)
summary(mod_logreg)

model_condlogreg = clogit(case ~ spontaneous + induced + strata(stratum), data = infert)
summary(model_condlogreg)
```



Note that the intercept cancels out in conditional logistic regression. It could vary by group and it would still cancel (similar to so-called fixed-effects models). Also, any variable that is constant within group will similarly cancel out.  This may be better understood when the (log) likelihood is expressed as follows for the case where the strata are balanced (i.e. 1:1 matching).

$$ \mathrm{L} = (X_{y=1} - X_{y=0})\beta$$
$$ \mathcal{L} = \ln(\frac{e^L}{1+e^L})$$

As such, anything constant within a strata would simply be 0.  


You might have noticed the call in the clogit command output, where it says `coxph(...)`.  That isn't an error, the conditional logit is equivalent to the stratified cox proportional hazards model where the survival time is simply 1 if the event is observed, or censored (1+) if not.

```{r survival}
coxph(Surv(rep(1, nrow(infert)), case) ~ spontaneous + induced + strata(stratum), data=infert)
```


In the case of 1:N matching, the denominator is based on the sum of all N non-event outcomes. If X represents the covariate values for which the event occurs, and Z for those in which it does not, for each strata:


$$ \mathcal{L} = \ln(\frac{e^{X_{y=1}\beta}}{\sum_{k=1}^N e^{Z_k\beta}})$$


### Bradley-Terry Model

The Bradley-Terry model (BT henceforth) is one in which we look to model pairwise rankings. For example, if one were to choose among various brands of some product they might select between two products at a time.  In the simplest case, the BT model posits the probability product $i$ is chosen over product $j$ as:

$$\pi_{i>j} = \frac{\exp(\beta_i)}{\exp(\beta_i)+\exp(\beta_j)}$$
$$\pi_{i>j} = \frac{\exp(\beta_i-\beta_j)}{1+\exp(\beta_i-\beta_j)}$$
$$\mathrm{Logit}(\pi_{i>j})  = \log(\frac{\pi_{i>j}}{1 - \pi_{i>j}}) = \log(\frac{\pi_{i>j}}{\pi_{j>i}}) = \beta_i - \beta_j$$


Thus it turns out the BT model has a connection to the standard logistic model, though we'll have to set up hte data in a specific manner for comparison. We start by creating a model matrix where each column is 1 if that item is chosen, -1 if it isn't, and zero if it is not considered. Our response in this case is simply positive values. 

```{r BTdatasetup, echo=F}
logreg = function(par, X, y){
  L = X %*% par
  -sum(dbinom(y, size=1, prob=plogis(L), log=T))
}
# create some data. -1 means loser/not chosen, 1 means chosen, 0 otherwise; 
# item 1 is most preferable
BTdat = data.frame(item1 = c(-1, rep(1,7), rep(0,4)),
                   item2 = c(rep(c(0,-1), e=4), 1,-1,-1,-1),
                   item3 = c(1,-1,-1,-1, rep(0,4), -1,1,1,1),
                   y=1)

DT::datatable(BTdat,rownames=F, style='bootstrap',  options=list(dom='t', pageLength=12), width=400)
```

```{r BTglm}
# glm output for comparison; no intercept; matrix columns are reordered for easier
# comparison; item1 is the reference group and so won't have a coefficient
glmmod = glm(y ~ -1 + ., data=BTdat[,c(2,3,1,4)], family=binomial)  
coef(glmmod)

# using logreg function in appendix;
out = optim(rep(0,3), logreg, X=as.matrix(BTdat[,-4]), y=BTdat$y, method='BFGS')
out$par                  # default chooses item 3 as reference
out$par[2:3]-out$par[1]  # now same as glm with item 1 as reference
```

Once constructed we can run the standard logistic model, and I do so with a custom function, and with the glm function.  For identification, one of the parameters must be set to zero, which glm just simply drops out.  By default the optim output settles on the third item as the reference (as would glm[^btaslogreg]).

For the BT model we need a different data structure, and will have a binary response where 1 represents that comparison the first comparison item was chosen. I show the first few rows here.


```{r BTdatasetup2, echo=F}
# create data format for BT2 package, y is 1 = winner
BTdat2 = rbind(data.frame(comp1= 'item1', comp2='item2', y=rep(1,4)),
               data.frame(comp1= 'item1', comp2='item3', y=c(0,1,1,1)),
               data.frame(comp1= 'item2', comp2='item3', y=c(1,0,0,0)))

BTdat2 = BTdat2 %>% 
  mutate(comp1=factor(comp1, levels=c('item1','item2','item3')),
         comp2=factor(comp2, levels=c('item1','item2','item3')))

DT::datatable(BTdat2,rownames=F, style='bootstrap',  options=list(dom='t', pageLength=6), width=300)
```

The following uses the <span class="pack">BradleyTerry2</span> package and compares the results to the standard logistic both with our custom function and the glm result.


```{r BTcomparison, echo=-7}
library(BradleyTerry2)
btmod = BTm(y, comp1, comp2, data = BTdat2)

# coefficients in the BT model are a glm binomial regression with appropriate
# coding of the model matrix, and represent the difference in the coefficient
# from the reference group coefficient.
data.frame(optim=out$par[2:3]-out$par[1], glm=coef(glmmod)[-3], BT=coef(btmod)) %>% round(5)
```


We can see that all results are the same.  Once again the output parameters from the model thus tells us the coefficient *difference* from some reference group coefficient.  Using the inverse logit transform tells us the probability of selecting that item relative to the reference group, but as noted previously, that transformation applied to any difference of the coefficients tells us that probability.  For example, with Item 1's parameter set to zero, the probability of choosing Item 1 vs. Item 2 is `plogis(0-(-2.83894))` `r round(plogis(0-(-2.83894)),3)`, which you can confirm by applying the <span class="func">fitted</span> function to the <span class="objclass">btmod</span>.  

Note also the BT model is generalizable to counts, just like binary logistic regression is a special case of binomial regression more generally.  Also, the BT model can handle ties, and choice specific covariates, but at that point we're in the realm of multinomial regression, so we'll turn to that now.





# More than two

In many situations the variables of interest have more than two categories, and as such the standard binary approach will not be the best approach[^mach]. In the simplest case, we might have the same interpretation, but as we will see things can be notably extended from that.


## Multinomial Logistic Regression


### A starting point

We'll start with another example from the [UCLA ATS website](http://www.ats.ucla.edu/stat/stata/dae/mlogit.htm) (the example there is done with Stata for those interested). 200 entering high school students make program choices: general program, vocational program and academic program. We will model their choice using their writing score as a proxy for scholastic ability and their socioeconomic status, a categorical variable of low, middle, and high values.  

```{r echo=3, eval=T}
# haven gets entirely too cute with what is a simple data set
# haven::write_dta(foreign::read.dta("http://www.ats.ucla.edu/stat/data/hsbdemo.dta"), 'logistic/data/hsbdemo.dta')
program = foreign::read.dta('data/hsbdemo.dta')
program$prog <- relevel(program$prog, ref = "academic")
```

As there are three categories we no longer use the glm function, which does not have the multinomial as a possible value to the 'family' argument.  There are a few packages available we could use, and here I'll use the <span class="pack">mlogit</span> package[^mnlogit].  I should also mention that it has a very extensive vignette that is worth perusal, at least part of which has been incorporated into later discussion.  

Let's run the model to predict program choice. Because of the <span class="pack">mlogit's</span> modeling flexibility, the data either has to have a specific structure or be told what the structure is. This oddly means that one can apparently not use it directly for what some might consider the standard multinomial model without pre-processing, and it's not the most intuitive approach to get it into the proper format with the <span class="func">mlogit.data</span> function. The gist is that the all choices must be represented for each individual, along with a variable that represents the actual choice made. Those familiar with mixed models, e.g. with longitudinal data, will note this as *long* format.

Original: 

```{r mlogit_data_orig, echo=FALSE}
DT::datatable(program[,1:7], options=list(dom='t', pageLength=5), style='bootstrap', rownames=F, width=600)
```

<br>
Long format:

```{r mlogit_data_proc, echo=F}
library(mlogit)

programLong = program %>% 
  select(id, prog, ses, write) %>% 
  mlogit.data(data=, shape='wide', choice='prog', id.var='id')
DT::datatable(programLong, options=list(dom='t', pageLength=5), style='bootstrap', width=600, rownames=F)
```

The program variable is now just a logical variable that notes which of the alternatives, i.e. the `alt` variable, was chosen. The reason for this data format is that it would allow for alternative specific variables to come into the model as well, something we will see later.

Keeping with our previous formulation, we can see the model as a *set* of logistic regression models for the probability of some category vs. a reference category.  Here we'll choose the reference category as academic.

$$\ln\left(\frac{\pi_{\textrm{prog=general}}}{\pi_{\textrm{prog=academic}}}\right) = b_{0\_\mathrm{gen}} + b_{1\_\mathrm{gen}}\mathrm{write} + b_{3\_\mathrm{gen}}\textrm{ses}_\textrm{middle} + b_{4\_\mathrm{gen}}\textrm{ses}_\textrm{high}$$
$$\ln\left(\frac{\pi_{\textrm{prog=vocation}}}{\pi_{\textrm{prog=academic}}}\right) = b_{0\_\mathrm{voc}} + b_{1\_\mathrm{voc}}\mathrm{write} + b_{3\_\mathrm{voc}}\textrm{ses}_\textrm{middle} + b_{4\_\mathrm{voc}}\textrm{ses}_\textrm{high}$$


So our interpretation will be as before, it's just that we'll have to interpret K-1 sets of parameters for K total categories. Let's go ahead and run the model.

```{r}
multi_mod = mlogit(prog ~ 1|write + ses, data=programLong)
summary(multi_mod)
```

Note that we get two sets of output. You can think of it as you would two separate binary logistic regressions, one where academic is the reference group and general the positive value, and another model that also has academic as the reference group and vocational as the positive value.  We see statistically notable effects of write and high ses (low ses is the reference category).  The general interpretation would similar to before, but sometimes the expontiated values are referred to *relative risk* rather than an odds ratio. It is no longer $\pi/(1-\pi)$, but a ratio of two relative probabilities (one group vs. an arbitrary reference group), hence the name relative risk (the 'risk' comes from biostats where the event of interest is usually something like death, disease, etc.)[^rrr].

As we move up one score on the variable write, we would see a decrease of `r round(coef(multi_mod)[3],3)` in the log probability of choosing the general program relative to the academic.  In other words, higher write scores are associated with a more likely choosing of the academic program instead of the general. The same goes for the vocational group, but it is stronger, i.e. we're even more likely to select the academic program over the vocational with an increase in write score.

It is important to note that the output doesn't tell us the overall effect of write is due to the relative nature of the model.  Let's plot the predictions so that we can understand it better. In the following, ses is fixed to be 'middle'.

```{r plotmultinom, echo=FALSE}
preddata = data.frame(write=c(replicate(3, seq(min(program$write), max(program$write), length.out=90))),
                      ses=factor('middle', levels=levels(program$ses)))
preddata = data.frame(preddata, predict(multi_mod, newdata=arrange(preddata, write), returnData=T))

preddata %>%
  select(-ses) %>%
  gather(key=outcome, value=probability, -write) %>%
  arrange(write) %>%
  ggplot(aes(x=write,y=probability, color=outcome)) +
  geom_line(lwd=1) +
  theme_trueMinimal()
```

So the general interpretation of the effect of write would be that higher scores increase the probability of choosing the academic program, decrease the probability of choosing the vocational program, and have no effect on choosing the general program.  Here are predictions for all three ses levels.


```{r plotmultinom2, fig.width=8, fig.height=3, echo=FALSE}
preddata = data.frame(write=c(replicate(3, seq(min(program$write), max(program$write), length.out=90))), 
                      ses=factor(rep(levels(program$ses), t=90), levels(program$ses)))
preddata = data.frame(preddata, predict(multi_mod, newdata=arrange(preddata, write), returnData=T))

preddata %>% 
  gather(key=outcome, value=probability, -write, -ses) %>% 
  ggplot(aes(x=write,y=probability, color=outcome)) +
  geom_line(lwd=1) +
  facet_grid(~ses) +
  theme_trueMinimal()
```



### Utility Maximization

In general we can consider the bi/multinomial logistic model from the perspective of utility maximization, in which a person (or whatever the unit of observation is) selects the choice that will provide most utility to them[^utility]. The probability a person will choose alternative $i$, $\pi_i$, given some utility over all other alternatives $j$, can be formulated as:

$$ \pi_i = \textrm{Prob}(U_i > U_{\forall j\neq i)}$$

Note that the actual value of utility doesn't matter, just the difference.  So comparing choice $i$ vs. some specific category $j$:

$$ \pi_i = \textrm{Prob}(U_i - U_j > 0)$$


The fact that only differences matter has implications for how we estimate the model. Consider the utility for category 1 and 2 we have the typical linear combination we've seen before.  Here we'll assume X is an individual level covariate, say age. There might be other covariates as well, but we'll just focus on this one.

$$U_1 = \dots + b^1_1\mathrm{Age} + \epsilon_1$$
$$U_2 = \dots + b^1_2\mathrm{Age} + \epsilon_2$$

If we try to estimate the probability U_2 - U_1 > 0, the age variable doesn't change for the individual and thus would drop out.  However, we can set the $b^1_1$ to zero and we end up with the following:

$$\begin{align}
U_1 &= \dots + \epsilon_1 \\
U_2 &= \dots + b_2\mathrm{Age} + \epsilon_2
\end{align}$$

And $b_2$ now represents the difference $b^1_2 - b^2_2$.  The interpretation of the coefficient isn't the effect of age on the utility of choice 2, but rather on the utility of choice 2 relative to choice 1.  We can also see this in terms of the predicted probabilities.  Recall our previous depiction for the [binary outcome case](#binpred).  Now the probability of selecting choice $i$ is:

$$ \frac{e^{X\beta_i}}{\Sigma^J_{j=1}e^{X\beta_j}}$$


If the first set of coefficients is zero, then it becomes:

$$ \pi_1 = \frac{1}{1 + \Sigma^J_{j=2}e^{X\beta}} \quad \tiny\textrm{(choice 1)}$$
$$ \pi_{i \neq 1} = \frac{e^{X\beta_i}}{1 + \Sigma^J_{j=2}e^{X\beta_j}}  \quad \tiny\textrm{(others)}$$



### Alternative specific covariates

Consider the following situation in which we have several possible outcomes[^mlogit] chosen by individuals.

- There are *alternative specific* variables $x_{ij}$ with generic coefficient $\beta$
- There are *individual specific* variables $z_i$ with alternative specific coefficients $\gamma_j$
- There are *alternative specific* variables $w_{ij}$ with alternative specific coefficients $\delta_j$

As an example, we have the following data on travel mode choice for travel between Sydney and Melbourne, Australia to help further clarify the nature these types of variables. We have a data frame containing 840 observations on 4 modes for 210 individuals.

- individual: Factor indicating individual with levels 1 to 200.
- mode: Factor indicating travel mode with levels "car", "air", "train", or "bus".
- choice: Factor indicating choice with levels "no" and "yes".
- wait: Terminal waiting time, 0 for car.
- vcost: Vehicle cost component.
- travel: Travel time in the vehicle.
- gcost: Generalized cost measure.
- income: Household income.
- size: Party size.

It looks like the following:

```{r, message=FALSE, echo=FALSE}
library(mlogit)
data(TravelMode, package='AER')
head(TravelMode, 10)
```

In this case income and size are individual specific variables. vcost and travel time are alternative specific. For vcost we want a generic coefficient and for travel time an alternative specific coefficient.

So the conceptual model for the *utility* or *satisfaction index* for alternative $j$ (with intercepts $\alpha$) is:
$$V_{ij} = \alpha_j + \beta x_{ij} + \gamma_jz_i + \delta_jw_{ij}$$

Common names for some models pertaining to such data are:

*multinomial logit*: in this case we have only individual specific covariates

*conditional logit*: we have only alternative specific covariates

*mixed logit*: has both kinds of variables.

Unfortunately, *conditional logit* also refers to a type of model for longitudinal data, and *mixed logit* might be short for a mixed effects model with random effects. Personally, I'm not sure why people refer to models based on their link function, as if that was descriptive enough. These models are sometimes used with other link functions, e.g. probit, and so the exact same model has to be called yet another name. But I digress. The point is you need to be careful and make sure the model is clear.

```{r}
library(mlogit)
data('Train', package='mlogit')
Tr <- mlogit.data(Train, choice = "choice", shape = "wide",
                     varying = 4:11, alt.levels = c(1,2), sep = "", id='id')
head(Tr)

library(dplyr)
Tr$price= Tr$price/100*2.20371
Tr$time = Tr$time/60

ml.train = mlogit(choice ~ price + time + change + comfort | 0, data=Tr)
summary(ml.train)
```


### Extensions

Nested logit



## Ordinal

### Extensions

gologit


## Other

### Distributions
As Bernoulli is to the binomial, so is the *categorical* distribution, commonly referred to as such in computer science, to the multinomial distribution. In other words, if only one choice among several is made (i.e. we're not dealing with counts), then we have the categorical distribution.

beta binomial

### Logit transformation
logistic transform = logit probability = softmax = sigmoid

### Probit

### IRT

### Logistic growth curves

### Loss functions

In the above we have assumed a standard likelihood approach


# Appendix

## Maximum likelihood estimation 

### Binomial logistic model

```{r logreg_maxlike, eval=T, collapse=TRUE}
logreg = function(par, X, y){
  L = X %*% par
  -sum(dbinom(y, size=1, prob=plogis(L), log=T))
}

mm = model.matrix(mod)
resp = admission$admit
inits = rep(0, 4)
out = optim(inits, logreg, X=mm, y=resp)
cbind(out$par, coefs) %>% round(2)
```


### Conditional logistic model

```{r condlogreg_maxlike, code_folding='hide', eval=T}
condlogreg = function(par, X, y){
  idx = y==1
  L1 = X[idx,] %*% par
  L2 = X[!idx,] %*% par
  L3 = (X[idx,] - X[!idx,]) %*% par
  
  # the following are all identical
  -sum(log(exp(L1)/(exp(L1)+exp(L2))))
  -sum(log(exp(L3)/(1+exp(L3))))
  -sum(dbinom(y[idx], size=1, prob=plogis(L3), log=T))
}


# create 1:1 matching
infert2 = infert %>% group_by(stratum) %>% slice(1:2)
mm = model.matrix(glm(case ~ spontaneous + induced, data = infert2, family = binomial))
mm = mm[,-1]   # no intercept
resp = infert2$case
inits = rep(0, 2)     # initial values
out = optim(inits, condlogreg, X=mm, y=resp)
coefs = coef(clogit(case ~ spontaneous + induced + strata(stratum), data = infert2))
cbind(out$par, coefs) %>% round(2)
```


### Multinomial logistic model

#### Individual varying only

```{r multinomMaxLike}
multinomregML <- function(par, X, y) {
  levs = levels(y)
  ref = levs[1]

  y0 = y==ref
  y1 = y==levs[2]
  y2 = y==levs[3]


  par = matrix(par, ncol=2)

  # V1 = X %*% par[1:4]
  # V2 = X %*% par[5:8]
  # ll = sum(-log(1 + exp(V1)+exp(V2))) + sum(V1[y1],V2[y2])  # more like mnlogit depiction

  V = X %*% par
  baseProbVec <- 1/(1 + rowSums(exp(V)))

  loglik = sum(log(baseProbVec))  + crossprod(c(V), c(y1, y2))
  loglik
}

out = optim(runif(8, -.1, .1), multinomregML, X=model.matrix(prog ~ ses + write, data = program),
             y=program$prog, control=list(maxit=1000, reltol=1e-12, ndeps=rep(1e-8, 8),
                                      trace=T, fnscale=-1, type=3),
             method='BFGS')
out$par

cbind(out$par, coef(multi_mod)[c(1,5,7,3,2,6,8,4)])

X=model.matrix(prog ~ ses + write, data = program)
y=program$prog
y1 = y=='general'
y2 = y=='vocation'
pars = matrix(out$par, ncol=2)
V = X %*% pars
acadprob = 1/(1+rowSums(exp(V)))
fitnonacad = exp(V) * matrix(rep(acadprob, 2), ncol = 2)
fits = cbind(acadprob, fitnonacad)
yind = model.matrix(~-1+prog, data=program)


# because dmultinom can't take matrix for prob
ll = 0
for (i in 1:200){
  ll = ll+dmultinom(yind[i,], size=1, prob=fits[i,], log=T)
}
ll
out$value



# stanmod = '
# data {
#   int K;
#   int N;
#   int D;
#   int y[N];
#   vector[D] x[N];
# }
# parameters {
#   matrix[K,D] beta;
# }
# model {
#   for (k in 1:K)
#     beta[k] ~ normal(0, 5);
#   for (n in 1:N)
#     y[n] ~ categorical(softmax(beta * x[n]));
# }'
# 
# datalist = list(x = X, y=as.integer(y), K=n_distinct(y), D=ncol(X))
# library(rstan)
# out = stan(model_code=stanmod, data=datalist)
```




[^subscript]: I drop the subscript to denote individual observations as it doesn't add clarity here.  You may treat it as a single observation or $N$ observations.  Later it will become more important.

[^stratum]: Technically one of the strata has a single case and control.

[^btaslogreg]: It would be more accurate to simply estimate two coefficients while fixing the reference to zero.

[^mach]: One place you will see the situation routinely ignored is with machine learning approaches, where 'multi-class' problems end up being multiple outcomes of 1 category vs. all others, or possibly all pairwise comparisons of categories.  In other words, it is just made into multiple binary classification problems.

[^mlogit]: This is a summary of a nicely laid out portion of the vignette for the mlogit package.

[^mnlogit]: See also the <span class="pack">mnlogit</span> package, which has a faster implementation.

[^rrr]: This is how Stata refers to the exponentiated coefficients.

[^utility]: The notion of *utility* doesn't really work so well if your outcome is a disease or similarly negatively oriented outcomes.  In that case, one can just think of it as a latent variable representing the likelihood of some category. 


# References

[Interpreting logistic regression in all its forms](http://www.ats.ucla.edu/stat/stata/library/sg124.pdf)

See [this](http://www.ats.ucla.edu/stat/r/dae/mlogit.htm) for more points on multinomial.

Kenneth Train, [Discrete Choice Methods with Simulation](http://eml.berkeley.edu/books/choice2.html)