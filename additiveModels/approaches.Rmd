---
title: <span style="color:#0085A1; font-size:5rem">Generalized Additive Models</span>
author: |
  | <a href="http://mclark--.github.io/"><span style="font-size:2rem">Michael Clark</span></a>
  | <span style="color:#00274c">Statistician Lead
  | Consulting for Statistics, Computing and Analytics Research
  | Advanced Research Computing </span>
date: '`r Sys.Date()`'
output:
  html_document:
    css: tufte-css-master/tufte.css
    highlight: pygments
    keep_md: no
    theme: cosmo
    toc: yes
    toc_float: yes
    # code_folding: show
  pdf_document:
    highlight: pygments
    toc: yes
always_allow_html: yes
---

#<a name='OtherApproaches'>Other Approaches</a>

<span class="newthought">This section will discuss</span> some ways to relate the generalized models above to other forms of nonlinear modeling approaches, some familiar and others perhaps less so.  In addition, I will note some extensions to GAMs to consider.

##<a name='relations'>Relation to Other Nonlinear Modeling Approaches</a>


###<a name='knownForm'>Known Form</a>
It<span class="marginnote">A general form of linear and nonlinear models: $$y = f(X,\beta)+\epsilon$$</span> should be noted that one can place generalized additive models under a general heading of *nonlinear models* whose focus may be on transformations of the outcome (as with generalized linear models), the predictor variables (polynomial regression and GAMs), or both (GAMs), in addition to those whose effects are nonlinear in the parameters <label for="sn-demo" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-demo" class="margin-toggle"/><span class="sidenote">For example, various theoretically motivated models in economics and ecology.</span>. The difference between the current presentation and those latter nonlinear models as distinguished in typical introductory statistical texts that might cover some nonlinear modeling, is that we simply don't know the form beforehand.

In cases where the form may be known, one can use an approach such as nonlinear least squares, and there is inherent functionality within a standard R installation, such as the \textcolor{red}{\texttt{nls}} function. As is the usual case, such functionality is readily extendable to a great many other analytic situations, e.g. the \textcolor{blue}{\emph{gnm}} for generalized nonlinear models or \textcolor{blue}{\emph{nlme}} for nonlinear mixed effects models.


##<a name='responseTransformation'>Response Transformation</a>

It is common practice, perhaps too common, to manually transform the response and go about things with a typical linear model.  While there might be specific reasons for doing so, the primary reason applied researchers seem to do so is to make the distribution 'more normal' so that regular regression methods can be applied, which stems from a misunderstanding of the assumptions of standard regression.  As an example, a typical transformation is to take the log, particularly to tame 'outliers' or deal with heteroscedasticity.  

While it was a convenience 'back in the day' because we didn't have software or computing power to deal with a lot of data situations aptly, this is definitely not the case now.  In many situations it would be better to, for example, conduct a generalized linear model with a log link or perhaps assume a different distribution for the response directly (e.g. skew-normal), and many tools allow researchers to do this with ease<label for="sn-demo" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-demo" class="margin-toggle"/><span class="sidenote">A lot of 'outliers' tend to magically go away with an appropriate choice of distribution for the data generating process.</span>.

There are still cases where one might focus on response transformation, just not so one can overcome some particular nuisance in trying to fit a linear regression.  An example might be in some forms of \emph{functional data analysis}, where we are concerned with some function of the response that has been measured on many occasions over time.


###<a name='blackbox'>The Black Box}</a>

\citet[Section 11.5]{venables_modern_2002} make an interesting classification of nonlinear models into those that are less flexible but under full user control (fully parametric)<span class="marginnote">One could probably make the case that most modeling is 'black box' for a great many researchers.</span>, and those that are \emph{black box} techniques that are highly flexible and fully automatic: stuff goes in, stuff comes out, but we're not privy to the specifics<label for="sn-demo" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-demo" class="margin-toggle"/><span class="sidenote">For an excellent discussion of these different approaches to understanding data see \citet{breiman_statistical_2001} and associated commentary. For some general packages outside of R that incorporate a more algorithmic approach to modeling, you might check out the <span class="pack">scikit-learn</span> module for Python.</span>.  

Two examples of the latter that they provide are \emph{projection pursuit} and \emph{neural net} models, though a great many would fall into such a heading.  Projection pursuit models are well suited to high dimensional data where dimension reduction is a concern.  One may think of an example where one uses a technique such as principal components analysis on the predictor set and then examines smooth functions of $M$ principal components. 

In the case of neural net models, one can imagine a model where the input units (predictor variables) are  weighted and summed to create hidden layer units, which are then essentially put through the same process to create outputs (see a simple example to the right).

<span class="marginnote"><img src='vis/nnet.png'> <br> A Neural Net Model</span>

One can see projection pursuit models as an example where a smooth function is taken of the components which make up the hidden layer.  Neural networks are highly flexible in that there can be any number of inputs, hidden layers, and outputs.  However, such models are very explicit in the black box approach.

Projection pursuit and neural net models are usually found among data mining/machine learning techniques any number of which might be utilized in a number of disciplines. Other more algorithmic/black box approaches include \emph{k-nearest-neighbors}, \emph{random forests}, \emph{support vector machines}, and various tweaks or variations thereof including boosting, bagging, bragging and other alliterative shenanigans<label for="sn-demo" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-demo" class="margin-toggle"/>See \citet{hastie_elements_2009} for an overview of such approaches.</span>. As Venables and Ripley note, generalized additive models might be thought of as falling somewhere in between the fully parametric and interpretable models of linear regression and black box techniques.  Indeed, there are algorithmic approaches which utilize GAMs as part of their approach.


##<a name='extensions'>Extensions</a>


##<a name='otherGams'>Other GAMs</a>

Note that just as generalized additive models are an extension of the generalized linear model, there are generalizations of the basic GAM beyond the settings described.  In particular, random effects can be dealt with in this context as they can with linear and generalized linear models, and there is an interesting connection between smooths and random effects in general <label for="sn-demo" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-demo" class="margin-toggle"/><span class="sidenote">\citet{wood_generalized_2006} has a whole chapter devoted to the subject.</span>.  Generalized additive models for location, scale, and shape (GAMLSS) allow for distributions beyond the exponential family \citet{rigby_generalized_2005}.  In addition there are boosted, ensemble and other machine learning approaches that apply GAMs as well<label for="sn-demo" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-demo" class="margin-toggle"/><span class="sidenote">See the \emph{\textcolor{blue}{GAMens}} package for example.</span>.  In short, there's plenty to continue to explore once one gets the hang of generalized additive models.


##<a name='gp'>Gaussian Processes: a Probabilistic Approach</a>

We can also approach modeling by using generalizations of the Gaussian distribution.  Where the Gaussian distribution is over vectors and defined by a mean vector and covariance matrix, the \emph{Gaussian Process} is over functions. A function $f$ is distributed as a Gaussian Process defined by a mean function $m$ and covariance function $k$. 
\medskip

$$f\sim \mathcal{GP}(m,k)$$

<span class="marginnote"><img src='vis/gp.png'> <br> Gaussian Process $y = \sin(x) + \text{noise}$.  The left graph shows functions from the prior distribution, the right shows the posterior mean function, 95\% confidence interval shaded, as well as specific draws from the posterior predictive mean distribution.</span>

In the Bayesian context we can define a prior distribution over functions and make draws from a posterior predictive distribution of $f$.  The reader is encouraged to consult \citet{rasmussen_gaussian_2006} for the necessary detail. The text is free for download \href{http://www.gaussianprocess.org/gpml/}{here}, and Rasmussen provides a nice and brief intro \href{http://link.springer.com/chapter/10.1007/978-3-540-28650-9_4}{here}.  I also have some R code for demonstration \href{https://github.com/mclark--/Miscellaneous-R-Code/blob/master/ModelFitting/gp Examples/gaussianprocessNoisy.R}{here} based on his Matlab code. 


Suffice it to say in this context, it turns out that generalized additive models with a tensor product or cubic spline smooth are maximum a posteriori (MAP) estimates of gaussian processes with specific covariance functions and a zero mean function. In that sense one might segue nicely to gaussian processes if familiar with additive models.