
# Appendix

## Data set descriptions

### McClelland 

#### Description

- McClelland et al. (2013) abstract 

> This study examined relations between children's attention span-persistence in preschool and later school achievement and college completion. Children were drawn from the Colorado Adoption Project using adopted and non-adopted children (N = 430). Results of structural equation modeling indicated that children's age 4 attention span-persistence significantly predicted math and reading achievement at age 21 after controlling for achievement levels at age 7, adopted status, child vocabulary skills, gender, and maternal education level. Relations between attention span-persistence and later achievement were not fully mediated by age 7 achievement levels. Logistic regressions also revealed that age 4 attention span-persistence skills significantly predicted the odds of completing college by age 25. The majority of this relationship was direct and was not significantly mediated by math or reading skills at age 7 or age 21. Specifically, children who were rated one standard deviation higher on attention span-persistence at age 4 had 48.7% greater odds of completing college by age 25. Discussion focuses on the importance of children's early attention span-persistence for later school achievement and educational attainment.

#### Reference

McClelland, Acock, Piccinin, Rheac, Stallings. (2013). Relations between preschool attention span-persistence and age 25 educational outcomes. [link](http://www.sciencedirect.com/science/article/pii/S0885200612000762) Note that there is only one age 25 outcome (college completion) and two age 21 outcomes.

The following models duplicate the paper results. Additional non-mediation models are provided for comparison.

```{r, eval=FALSE}
modReading = "
  read21 ~ rr*read7 + ar21*attention4 + vocab4 + male + adopted + momed
  read7 ~ ar7*attention4

  # in
  att4_read21 := ar7*rr
"
reading  = sem(modReading, data=mcclelland, missing='fiml', mimic = 'Mplus', std.ov=TRUE, se ='boot')
summary(reading, rsquare=TRUE)

modRead = "
  read21 ~ read7 + attention4 + vocab4 + male + adopted + momed

"
readnomed  = sem(modread, data=mcclelland, missing='fiml', mimic = 'Mplus', std.ov=TRUE, se ='boot')
AIC(read, readnomed)


modMath = "
  math21 ~ mm*math7 + am21*attention4 + vocab4 + male + adopted + momed
  math7 ~ am7*attention4

  # in
  att4_math21 := am7*mm
"
math  = sem(modMath, data=mcclelland, missing='fiml', mimic = 'Mplus', std.ov=TRUE, se ='boot')
summary(math, rsquare=TRUE, fit=T)

modMath = "
  math21 ~ math7 + attention4 + vocab4 + male + adopted + momed

"
mathnomed  = sem(modMath, data=mcclelland, missing='fiml', mimic = 'Mplus', std.ov=TRUE, se ='boot')
AIC(math, mathnomed)
```


### National Longitudinal Survey of Youth (1997, NLSY97)

#### Description

NLSY97 consists of a nationally representative sample of approximately 9,000 youths who were 12 to 16 years old as of December 31, 1996. Round 1 of the survey took place in 1997. In that round, both the eligible youth and one of that youth's parents received hour-long personal interviews. In addition, during the screening process, an extensive two-part questionnaire was administered that listed and gathered demographic information on members of the youth's household and on his or her immediate family members living elsewhere. Youths are interviewed on an annual basis. 

The NLSY97 is designed to document the transition from school to work and into adulthood. It collects extensive information about youths' labor market behavior and educational experiences over time. Employment information focuses on two types of jobs, "employee" jobs where youths work for a particular employer, and "freelance" jobs such as lawn mowing and babysitting. These distinctions will enable researchers to study effects of very early employment among youths. Employment data include start and stop dates of jobs, occupation, industry, hours, earnings, job search, and benefits. Measures of work experience, tenure with an employer, and employer transitions can also be obtained. Educational data include youths' schooling history, performance on standardized tests, course of study, the timing and types of degrees, and a detailed account of progression through post-secondary schooling.

#### Reference

[Bureau of Labor Statistics](http://www.bls.gov/nls/nlsy97.htm)


### Wheaton 1977 data 

#### Description

> Longitudinal data to develop a model of the stability of alienation from 1967 to 1971, accounting for socioeconomic status as a covariate. Each of the three factors have two indicator variables, SES in 1966 is measured by education and occupational status in 1966 and alienation in both years is measured by powerlessness and anomia.

#### Reference

Wheaton, B., Muthen B., Alwin, D., & Summers, G., 1977, "Assessing reliability and stability in panel models", in D. R. Heise (Ed.), _Sociological Methodology 1977_ (pp. 84-136), San Francisco: Jossey-Bass, Inc. 


### Old Faithful

#### From the R helpfile

Waiting time between eruptions and the duration of the eruption for the Old Faithful geyser in Yellowstone National Park, Wyoming, USA. A closer look at faithful$eruptions reveals that these are heavily rounded times originally in seconds, where multiples of 5 are more frequent than expected under non-human measurement. For a better version of the eruption times, see the example below.

There are many versions of this dataset around: Azzalini and Bowman (1990) use a more complete version.

### Harman 1974

#### Description

A correlation matrix of 24 psychological tests given to 145 seventh and eight-grade children in a Chicago suburb by Holzinger and Swineford.

#### Reference

Harman, H. H. (1976) Modern Factor Analysis, Third Edition Revised, University of Chicago Press, Table 7.4.




## Terminology in SEM

SEM as it is known has been widely used in psychology and education for decades, while other disciplines have developed and advanced techniques that are related, but would not typically call them SEM.  The following will be expanded over time.

**Latent variable**: an unobserved or hidden variable.  It's specific interpretation will depend on the modeling context.

**Factor Analysis**: in the SEM literature, this refers to a latent variable (measurement) model to assess the underlying construct behind the correlations among a set of observed variables.  Elsewhere it may refer to a very broad family of matrix factorization techniques that would include things like principal components analysis, non-negative matrix factorization, etc.

**Exploratory vs. Confirmatory**: This distinction is problematic.  Science and data analysis is inherently exploratory, and most who use SEM do some model exploration as they would with any other model.  Some SEM models have more constraints than others, but that does not require a separate name or way of thinking about the model.

**Mediation** and moderation:  These mean different things, both straightforward, and which is utilized in a model should be based on theoretical notions.

- Mediation: an indirect effect, e.g. A->B->C, A has an indirect effect on C. A can have a direct effect on C too. 
- Moderation: an interaction (the same ones utilized in a standard regression modeling)

**Fit**: Model fit is something very difficult to ascertain in SEM, and notoriously problematic in this setting, where all proposed cutoffs for a good fit are ultimately arbitrary.  Even if one had most fit indices suggesting a 'good' fit, there's little indication the model has predictive capability.

**Endo/Exogenous**: Endogenous variables are determined by other variables, exogenous variables have no analyzed causes.

**Disturbance**: residual variance. Includes measurement error and unknown causes.

**Mixture Models**: models using categorical latent variables.

## Causal Bias

I figure I should note my stance on soi-disant *causal modeling* so that whatever I might say in this document is taken with the appropriate context. What follows is more or less a philosophical stance, perhaps a na√Øve and not very well developed one at that, but one that I think is a safer perspective than others commonly held regarding causes and statistics. 


To begin, no statistical model by itself will ever provide evidence of causal effects. This is a fact that cannot be disputed.  Statistical models are inherently probabilistic, atheoretical in and of themselves, and wholly dependent on the data collected. No amount of fancy analysis or double-blind randomization will change the fact that in the end you have a probabilistic result, and one that is highly dependent on many assumptions both statistical and theoretical, as well as nuances of the data.  If you are using SEM or other approach to determine causal effects you will thus be unsuccessful, and as such, you should not be using them if that is the primary reason for doing so.

Philosophically speaking, I also don't think science can prove causal relations, and that's setting aside the fact that we've been debating the nature of causality for thousands of years and have yet to come to a conclusion about it.  As a starting example, there is nothing that convinces me in the least that smoking causes lung cancer, and that is because it doesn't.  If it did, everyone who ever smoked would have lung cancer.  This a deterministic notion of causality, and there are others, but it is the one I think is used in everyday parlance. Using a probabilistic approach, e.g. a smoker is more likely to have cancer, just serves to emphasize the uncertainty that exists in the underlying model.  I'm perfectly okay with this, but many seem uncomfortable with it.  I might even say that smoking has an 'effect' on the likelihood of getting lung cancer. But all we have in the data are those that have lung cancer or not, and there is no uncertainty about them having it, nor does our knowledge of their smoking habits change the fact of whether they do.

As another example, you can randomly assign people who have cancer to two groups- in one they take an aspirin every day, in the other they drink orange juice every day. You may then find that they are equally effective in terms of remission rates, but it would be silly to think the 'treatments' had any causal effect at all, even though the effects could be non-zero. Randomization, which is assumed by many to have the magical ability to confer causality on a scientific endeavor, in no way helps the situation either.  Modern health science is not removed from this issue, or even far removed from this example, and regularly finds 'effects' of drugs or behavior that have no causal relation to the phenomenon under study. This is especially the case with psychological phenomena, many of which to this day still have little or no tie to operational definitions based on physiology rather than behavior.  As yet another example, whether or not you get the flu depends first and foremost on whether or not you contract the (live) virus, not whether or not you get the flu shot (though I will admit you may be more likely to catch it by going to the doctor's office in the first place).  One of the main reasons health science results are regularly overturned is precisely because people assign causal effects to things that have no causal relation to the subject under study.  If you don't want the flu, live in a hyperbaric chamber.

I personally think it's misguided to think that a goal of science is to establish causal relations.  Its primary tool for weighing evidence, i.e. statistical modeling, certainly cannot do that. As much as we want control, which is the only thing that could establish causality, it eludes us.  *Ceteris peribus* can only work for what is included in our models and no more. On the practical side, few seem to be engaging in science engaging in it for reasons of determining causal effects (except perhaps in the discussion sections of papers).


### Prediction

A well-worn data set on which to test classification algorithms is the [MNIST hand written digits data](https://en.wikipedia.org/wiki/MNIST_database).  The methods use the pixel gray-scale information to classify an image of what are originally handwritten digits as being one of the values 0-9.  Modern approaches can get accurate classification *on test data* with less than 1% error.  If one's goal is to understand why the digits are the way they are, this model cannot help you. And yet, a statistical approach can be very successful while still having nothing to say about the causal mechanisms influencing the target variable we wish to speak about. 

If you can predict something with 99% accuracy, how much do you really care about the underlying causal reality? But this is in fact what I think is the primary goal of scientific endeavor, i.e. accurate prediction. It definitely is not about 'true' parameters and p-values.  The models are wrong, but they can work, and we'd prefer those that work better. That is something science can provide.  Models can even correspond to 'reality' as we currently know it, but we all know that knowledge of reality changes as well, sometimes due to science. Peirce figured this out long ago. I'd personally rather have something that works than have one of the 'true causes' and a model that leaves me no better than guessing.  Let's say you have a 'causal' model and another model that uses other covariates, and yet they both predict equally well. What would be the basis for preferring one over the other? What would tangibly be gained in the preference?


### Chance

Another related point, *chance* definitely does not cause anything. There are mechanisms you do not know about contributing to the variability in the subject matter under study, but nothing from your results are *due to chance*.  Statistical models will always have uncertainty regarding them, and how much or how little there is depends on many things.  But just because we don't know what's going on, we cannot put the unknown as due to some magical notion akin to coincidence.

### Other

Even as I write this, there are people, who have been taken seriously in other contexts, actively devoting time, money and energy to determine a. that our current existence is a simulation, and b. to find a way to 'break out of it' (I suspect this has to do with them taking their experience at Burning Man too seriously).  However useless an endeavor this may be, if it was a simulation, where would any theory about causality reside then?


## Resources

This list serves only as a starting point, though may be added to over time.


### Graphical Models

[Judea Pearl's website](http://bayes.cs.ucla.edu/jp_home.html): Includes papers and technical reports.

[UseR Series](http://link.springer.com/bookseries/6991): Contains texts on graphical models, Bayesian networks, and network analysis.


### Potential Outcomes

[Imai's website](http://imai.princeton.edu/projects/mechanisms.html): Papers and other info.


### Measurement Models

[Personality Project](http://personality-project.org/index.html): William Revelle's website and text on psychometric theory.


### SEM

Kline, Rex. *Principles and Practice of Structural Equation Modeling*. A very applied introduction that covers a lot of ground.  The latest edition finally includes explicit discussion of the more general graphical modeling framework within which SEM resides.

Beaujean, A. A. (2014). [Latent variable modeling using R: A step by step guide](http://blogs.baylor.edu/rlatentvariable/). New York, NY: Routledge/Taylor and Francis. Lavaan based guide to SEM


### lavaan

[lavaan website](http://lavaan.ugent.be/)

[Tutorial](http://lavaan.ugent.be/tutorial/tutorial.pdf) 

[Bayesian estimation with lavaan](https://cran.r-project.org/package=blavaan)

[Complex surveys with lavaan](https://cran.r-project.org/package=lavaan.survey)

[Interactive lavaan](https://github.com/kylehamilton/lavaan.shiny)


### Other SEM

[semTools]: Excellent set of tools for reliability assessment, measurement invariance, fit, simulation etc.

[semPlot]: Visualize your lavaan models.
