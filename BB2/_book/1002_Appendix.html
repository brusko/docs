<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta property="og:title" content="Bayesian Basics" />
<meta property="og:type" content="book" />


<meta property="og:description" content="An introduction to Bayesian data analysis." />
<meta name="github-repo" content="m-clark/Workshops" />


<meta name="date" content="2016-11-22" />

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

<meta name="description" content="An introduction to Bayesian data analysis.">

<title>Bayesian Basics</title>

<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>


<link rel="stylesheet" href="toc.css" type="text/css" />
<link rel="stylesheet" href="..\css\tufte_bookdown\mytufte.css" type="text/css" />

</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li><a href="index.html#home"><span style="color:transparent">Home</span></a></li>
<li class="has-sub"><a href="00_preface.html#preface">Preface</a><ul>
<li><a href="00_preface.html#prerequisites">Prerequisites</a></li>
</ul></li>
<li class="has-sub"><a href="01_intro.html#introduction">Introduction</a><ul>
<li class="has-sub"><a href="01_intro.html#bayesian-probability">Bayesian Probability</a><ul>
<li><a href="01_intro.html#conditional-probability-bayes-theorem">Conditional probability &amp; Bayes theorem</a></li>
</ul></li>
</ul></li>
<li class="has-sub"><a href="02_example.html#a-hands-on-example">A Hands-on Example</a><ul>
<li><a href="02_example.html#prior-likelihood-posterior-distributions">Prior, likelihood, &amp; posterior distributions</a></li>
<li><a href="02_example.html#prior">Prior</a></li>
<li><a href="02_example.html#likelihood">Likelihood</a></li>
<li><a href="02_example.html#posterior">Posterior</a></li>
<li><a href="02_example.html#posterior-predictive">Posterior predictive</a></li>
</ul></li>
<li class="has-sub"><a href="03_models.html#regression-models">Regression Models</a><ul>
<li><a href="03_models.html#example-linear-regression-model">Example: Linear Regression Model</a></li>
<li class="has-sub"><a href="03_models.html#setup">Setup</a><ul>
<li><a href="03_models.html#stan-code">Stan Code</a></li>
<li><a href="03_models.html#running-the-model">Running the Model</a></li>
</ul></li>
</ul></li>
<li><a href="1000_Conclusion.html#summary">Summary</a></li>
<li><a href="1001_references.html#references">References</a></li>
<li class="has-sub"><a href="1002_Appendix.html#appendix">Appendix</a><ul>
<li class="has-sub"><a href="1002_Appendix.html#maximum-likelihood-review">Maximum Likelihood Review</a><ul>
<li><a href="1002_Appendix.html#example">Example</a></li>
</ul></li>
<li><a href="1002_Appendix.html#linear-model">Linear Model</a></li>
<li><a href="1002_Appendix.html#binomial-likelihood-example">Binomial Likelihood Example</a></li>
<li><a href="1002_Appendix.html#modeling-languages">Modeling Languages</a></li>
</ul></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="appendix" class="section level1">
<h1>Appendix</h1>
<p>NOTE EVAL AND CACHE FALSE UNTIL WE GET TO THIS SECTION. CHange back <code>startvals[1]</code> inline to r.</p>
<div id="maximum-likelihood-review" class="section level2">
<h2>Maximum Likelihood Review</h2>
<p>This is a very brief refresher on <span class="emph">maximum likelihood estimation</span> using a standard regression approach as an example, and more or less assumes one hasn’t tried to roll their own such function in a programming environment before. Given the likelihood’s role in Bayesian estimation and statistics in general, and the ties between specific Bayesian results and maximum likelihood estimates one typically comes across, I figure one should be comfortable with some basic likelihood estimation.</p>
<p>In the standard model setting we attempt to find parameters <span class="math inline">\(\theta\)</span> that will maximize the probability of the data we actually observe<label for="sn-demo" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-demo" class="margin-toggle"/><span class="sidenote">The principle of maximum likelihood.</span>. We’ll start with an observed random target vector <span class="math inline">\(y\)</span> with <span class="math inline">\(i...N\)</span> independent and identically distributed observations and some data-generating process underlying it <span class="math inline">\(f(\cdot|\theta)\)</span>. We are interested in estimating the model parameter(s), <span class="math inline">\(\theta\)</span>, that would make the data most likely to have occurred. The probability density function for <span class="math inline">\(y\)</span> given some particular estimate for the parameters can be noted as <span class="math inline">\(f(y_i|\theta)\)</span>. The joint probability distribution of the (independent) observations given those parameters, <span class="math inline">\(f(y_i|\theta)\)</span>, is the product of the individual densities, and is our <em>likelihood function</em>. We can write it out generally as: <span class='marginnote'><span class="imgbigger"><img src="figure/maxLikeNormalCompareLLforDifferentMeans.svg" style="display:block; margin: 0 auto; width:90%"></span></p>
<p><span class="math display">\[\mathcal{L}(\theta) = \prod_{i=1}^N f(y_i|\theta)\]</span></p>
<p>Thus the <em>likelihood</em> for one set of parameter estimates given a fixed set of data y, is equal to the probability of the data given those (fixed) estimates. Furthermore we can compare one set, <span class="math inline">\(\mathcal{L}(\theta_A)\)</span>, to that of another, <span class="math inline">\(\mathcal{L}(\theta_B)\)</span>, and whichever produces the greater likelihood would be the preferred set of estimates. We can get a sense of this with the graph to the right, based on a single parameter, Poisson distributed variable. The data is drawn from a variable with mean <span class="math inline">\(\theta=5\)</span>. We note the calculated likelihood increases as we estimate values for <span class="math inline">\(\theta\)</span> closer to <span class="math inline">\(5\)</span>.</p>
<p>For computational reasons we instead work with the sum of the natural log probabilities<label for="sn-demo" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-demo" class="margin-toggle"/><span class="sidenote">Math refresher on logs: <code>log(A*B) = log(A)+log(B)</code>. So summing the log probabilities will result in the same values for <span class="math inline">\(\theta\)</span>, but won’t result in extremely small values that will break our computer.</span>, and thus the <em>log likelihood</em>:</p>
<p><span class="math display">\[\ln\mathcal{L}(\theta) = \sum_{i=1}^N \ln[f(y_i|\theta)]\]</span></p>
<p>Concretely, we calculate a log likelihood for each observation and then sum them for the total likelihood for parameter(s) <span class="math inline">\(\theta\)</span>.</p>
<p>The likelihood function incorporates our assumption about the sampling distribution of the data given some estimate for the parameters. It can take on many forms and be notably complex depending on the model in question, but once specified, we can use any number of optimization approaches to find the estimates of the parameter that make the data most likely. As an example, for a normally distributed variable of interest we can write the log likelihood as follows:</p>
<p><span class="math display">\[\ln\mathcal{L}(\theta) = \sum_{i=1}^N \ln[\frac{1}{\sqrt{2\pi\sigma^2}}\exp(-\frac{(y-\mu)^2}{2\sigma^2})]\]</span></p>
<div id="example" class="section level3">
<h3>Example</h3>
<p>In the following we will demonstrate the maximum likelihood approach to estimation for a simple setting incorporating a normal distribution where we estimate the mean and variance/sd for a set of values <span class="math inline">\(y\)</span><label for="sn-demo" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-demo" class="margin-toggle"/><span class="sidenote">Of course we could just use the sample estimates, but this is for demonstration.</span>. First the data is created, and then we create the function that will compute the log likelihood. Using the built in R distributions<label for="sn-demo" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-demo" class="margin-toggle"/><span class="sidenote">Type <code>?Distributions</code> at the console for some of the basic R distributions available.</span> makes it fairly straightforward to create our own likelihood function and feed it into an optimization function to find the best parameters. We will set things up to work with the <span class="pack">bbmle</span> package, which has some nice summary functionality and other features. However, one should take a glance at <span class="func">optim</span> and the other underlying functions that do the work.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># for replication</span>
<span class="kw">set.seed</span>(<span class="dv">1234</span>)

<span class="co"># create the data</span>
y =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">1000</span>, <span class="dt">mean=</span><span class="dv">5</span>, <span class="dt">sd=</span><span class="dv">2</span>)
startvals =<span class="st"> </span><span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>)

<span class="co"># the log likelihood function</span>
LL =<span class="st"> </span>function(<span class="dt">mu=</span>startvals[<span class="dv">1</span>], <span class="dt">sigma=</span>startvals[<span class="dv">2</span>]){
  ll =<span class="st"> </span>-<span class="kw">sum</span>(<span class="kw">dnorm</span>(y, <span class="dt">mean=</span>mu, <span class="dt">sd=</span>sigma, <span class="dt">log=</span>T))
  <span class="kw">message</span>(<span class="kw">paste</span>(mu, sigma, ll))
  ll
}</code></pre></div>
<p>The <span class="func">LL</span> function takes starting points for the parameters as arguments, in this case we call them <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>, which will be set to <code>startvals[1]</code> and <code>startvals[2]</code> respectively. Only the first line (ll = -sum…) is actually necessary, and we use <span class="func">dnorm</span> to get the density for each point<label for="sn-demo" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-demo" class="margin-toggle"/><span class="sidenote">Much more straightforward than writing the likelihood function as above.</span>. Since this optimizer is by default minimization, we reverse the sign of the sum so as to minimize the negative log likelihood, which is the same as maximizing the likelihood. Note that the bit of other code just allows you to see the estimates as the optimization procedure searches for the best values. I do not show that here but you’ll see it in your console.</p>
<p>We are now ready to obtain maximum likelihood estimates for the parameters. For the <span class="func">mle2</span> function we will need the function we’ve created, plus other inputs related to that function or the underlying optimizing function used (by default <span class="func">optim</span>). In this case we will use an optimization procedure that will allow us to set a lower bound for <span class="math inline">\(\sigma\)</span>. This isn’t strictly necessary, but otherwise you would get get warnings and possibly lack of convergence if negative estimates for <span class="math inline">\(\sigma\)</span> were allowed<label for="sn-demo" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-demo" class="margin-toggle"/><span class="sidenote">An alternative approach would be to work with the log of <span class="math inline">\(\sigma\)</span> which can take on negative values, and then convert it back to the original scale.</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(bbmle)
<span class="co"># using optim, and L-BFGS-B so as to contrain sigma to be positive by setting the lower bound at zero</span>
mlnorm =<span class="st">  </span><span class="kw">mle2</span>(LL, <span class="dt">method=</span><span class="st">&quot;L-BFGS-B&quot;</span>, <span class="dt">lower=</span><span class="kw">c</span>(<span class="dt">sigma=</span><span class="dv">0</span>)) 
mlnorm

<span class="co"># compare to an intercept only regression model</span>
<span class="kw">summary</span>(<span class="kw">lm</span>(y~<span class="dv">1</span>))</code></pre></div>
<p>We can see that the ML estimates are the same<label for="sn-demo" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-demo" class="margin-toggle"/><span class="sidenote">Actually there is a difference between the sigma estimates in that OLS estimates are based on a variance estimate divided by <span class="math inline">\(N-1\)</span> while the MLE estimate has a divisor of <span class="math inline">\(N\)</span>.</span> as the intercept only model estimates, which given the sample size are close to the true values.</p>
<p>In terms of the parameters we estimate, in the typical case of two or more parameters we can think of a <span class="emph">likelihood surface</span> that represents the possible likelihood values given any particular set of estimates. Given some starting point, the optimization procedure then travels along the surface looking for a minimum/maximum point<label for="sn-demo" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-demo" class="margin-toggle"/><span class="sidenote">Which is equivalent to finding the point where the slope of the tangent line to some function, i.e. the derivative, to the surface is zero. The derivative, or gradient in the case of multiple parameters, of the likelihood function with respect to the parameters is known as the <span class="emph">score function</span>.</span>. For simpler settings such as this, we can visualize the likelihood surface and its minimum point. The optimizer travels along this surface until it finds a minimum. I also plot the the path of the optimizer from a top down view. The large blue dot noted represents the minimum negative log likelihood.</p>
<p>Please note that there are many other considerations in optimization completely ignored here, but for our purposes and the audience for which this is intended, we do not want to lose sight of the forest for the trees. We now move next to a slightly more complicated regression example. <span class='marginnote'><span class="imgbigger"><img src="figure/mlnormLikelihoodSurface.png" style="display:block; margin: 0 auto;"></span></p>
<p><span class='marginnote'><span class="imgbigger"><img src="figure/mlnorm.svg" style="display:block; margin: 0 auto;"> <br> A bit of jitter was added to the points to better see what’s going on.</span></p>
</div>
</div>
<div id="linear-model" class="section level2">
<h2>Linear Model</h2>
<p>In the standard regression context, our expected value for the target comes from our linear predictor, i.e. the weighted combination of our explanatory variables, and we estimate the regression weights/coefficients and possibly other relevant parameters. We can expand our previous example to the standard linear model without too much change. In this case we estimate a mean for each observation, but otherwise assume the variance is constant across observations. Again we first construct some data so that we know exactly what to expect, then write out the likelihood function with starting parameters. As we need to estimate our intercept and coefficient for the X predictor (collectively referred to as <span class="math inline">\(\beta\)</span>), we can can think of our likelihood explicitly as before:</p>
<p><span class="math display">\[\ln\mathcal{L}(\beta, \sigma^2) = \sum_{i=1}^N \ln[\frac{1}{\sqrt{2\pi\sigma^2}}\exp(-\frac{(y-X\beta)^2}{2\sigma^2})]\]</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># for replication</span>
<span class="kw">set.seed</span>(<span class="dv">1234</span>)

<span class="co"># predictor</span>
X =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">1000</span>)

<span class="co"># coefficients for intercept and predictor</span>
theta =<span class="st"> </span><span class="kw">c</span>(<span class="dv">5</span>,<span class="dv">2</span>) 

<span class="co"># add intercept to X and create y with some noise</span>
y =<span class="st"> </span><span class="kw">cbind</span>(<span class="dv">1</span>,X)%*%theta +<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">1000</span>, <span class="dt">sd=</span><span class="fl">2.5</span>)

regLL =<span class="st"> </span>function(<span class="dt">sigma=</span><span class="dv">1</span>, <span class="dt">Int=</span><span class="dv">0</span>, <span class="dt">b1=</span><span class="dv">0</span>){
  coefs =<span class="st"> </span><span class="kw">c</span>(Int, b1)
  mu =<span class="st"> </span><span class="kw">cbind</span>(<span class="dv">1</span>,X)%*%coefs
  ll =<span class="st"> </span>-<span class="kw">sum</span>(<span class="kw">dnorm</span>(y, <span class="dt">mean=</span>mu, <span class="dt">sd=</span>sigma, <span class="dt">log=</span>T))
  
  <span class="kw">message</span>(<span class="kw">paste</span>(sigma, Int, b1, ll))
  ll
}

<span class="kw">library</span>(bbmle)
mlopt =<span class="st">  </span><span class="kw">mle2</span>(regLL, <span class="dt">method=</span><span class="st">&quot;L-BFGS-B&quot;</span>, <span class="dt">lower=</span><span class="kw">c</span>(<span class="dt">sigma=</span><span class="dv">0</span>)) 
<span class="kw">summary</span>(mlopt)
<span class="co"># plot(profile(mlopt), absVal=F)</span>

modlm =<span class="st"> </span><span class="kw">lm</span>(y~X)
<span class="kw">summary</span>(modlm)
-<span class="dv">2</span>*<span class="kw">logLik</span>(modlm)</code></pre></div>
<p>As before, our estimates and final log likelihood value are about where they should be, and reflect the lm output. The visualization becomes more difficult, but we can examine slices similar to the previous plot. <span class='marginnote'><span class="imgbigger"><img src="figure/mlreg.svg" style="display:block; margin: 0 auto;"></span></p>
<p>To move to generalized linear models, very little changes of the process outside of the distribution assumed and that we are typically modeling a function of the target variable (e.g. <span class="math inline">\(\log(y)=X\beta; mu = e^{X\beta}\)</span>).</p>
</div>
<div id="binomial-likelihood-example" class="section level2">
<h2>Binomial Likelihood Example</h2>
<p>This regards the example seen in the early part of the document with the hands-on example.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x1 =<span class="st"> </span><span class="kw">rbinom</span>(<span class="dv">1000</span>, <span class="dt">size=</span><span class="dv">10</span>, <span class="dt">p=</span>.<span class="dv">5</span>)
x2 =<span class="st"> </span><span class="kw">rbinom</span>(<span class="dv">1000</span>, <span class="dt">size=</span><span class="dv">10</span>, <span class="dt">p=</span>.<span class="dv">85</span>)

binomLL =<span class="st"> </span>function(theta, x) {
  -<span class="kw">sum</span>(<span class="kw">dbinom</span>(x, <span class="dt">size=</span><span class="dv">10</span>, <span class="dt">p=</span>theta, <span class="dt">log=</span>T))
}

<span class="kw">optimize</span>(binomLL, <span class="dt">x=</span>x1, <span class="dt">lower=</span><span class="dv">0</span>, <span class="dt">upper=</span><span class="dv">1</span>); <span class="kw">mean</span>(x1)
<span class="kw">optimize</span>(binomLL, <span class="dt">x=</span>x2, <span class="dt">lower=</span><span class="dv">0</span>, <span class="dt">upper=</span><span class="dv">1</span>); <span class="kw">mean</span>(x2)</code></pre></div>
</div>
<div id="modeling-languages" class="section level2">
<h2>Modeling Languages</h2>
<p>I will talk only briefly about a couple of the modeling language options available, as you will have to make your own choice among many.</p>

</div>
</div>
<p style="text-align: center;">
<a href="1001_references.html"><button class="btn btn-default">Previous</button></a>
</p>
</div>
</div>



</body>
</html>
