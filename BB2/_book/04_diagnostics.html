<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta property="og:title" content="Bayesian Basics" />
<meta property="og:type" content="book" />


<meta property="og:description" content="An introduction to Bayesian data analysis." />
<meta name="github-repo" content="m-clark/Workshops" />


<meta name="date" content="2016-11-25" />

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

<meta name="description" content="An introduction to Bayesian data analysis.">

<title>Bayesian Basics</title>

<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>


<link rel="stylesheet" href="toc.css" type="text/css" />
<link rel="stylesheet" href="..\css\tufte_bookdown\mytufte.css" type="text/css" />

</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li><a href="index.html#home"><span style="color:transparent">Home</span></a></li>
<li class="has-sub"><a href="00_preface.html#preface">Preface</a><ul>
<li><a href="00_preface.html#prerequisites">Prerequisites</a></li>
</ul></li>
<li class="has-sub"><a href="01_intro.html#introduction">Introduction</a><ul>
<li class="has-sub"><a href="01_intro.html#bayesian-probability">Bayesian Probability</a><ul>
<li><a href="01_intro.html#conditional-probability-bayes-theorem">Conditional probability &amp; Bayes theorem</a></li>
</ul></li>
</ul></li>
<li class="has-sub"><a href="02_example.html#a-hands-on-example">A Hands-on Example</a><ul>
<li><a href="02_example.html#prior-likelihood-posterior-distributions">Prior, likelihood, &amp; posterior distributions</a></li>
<li><a href="02_example.html#prior">Prior</a></li>
<li><a href="02_example.html#likelihood">Likelihood</a></li>
<li><a href="02_example.html#posterior">Posterior</a></li>
<li><a href="02_example.html#posterior-predictive">Posterior predictive</a></li>
</ul></li>
<li class="has-sub"><a href="03_models.html#regression-models">Regression Models</a><ul>
<li><a href="03_models.html#example-linear-regression-model">Example: Linear Regression Model</a></li>
<li class="has-sub"><a href="03_models.html#setup">Setup</a><ul>
<li><a href="03_models.html#stan-code">Stan Code</a></li>
<li><a href="03_models.html#running-the-model">Running the Model</a></li>
</ul></li>
</ul></li>
<li class="has-sub"><a href="04_diagnostics.html#model-checking-diagnostics">Model Checking &amp; Diagnostics</a><ul>
<li class="has-sub"><a href="04_diagnostics.html#monitoring-convergence">Monitoring Convergence</a><ul>
<li><a href="04_diagnostics.html#visual-inspection-traceplot-densities">Visual Inspection: Traceplot &amp; Densities</a></li>
<li><a href="04_diagnostics.html#statistical-measures">Statistical Measures</a></li>
<li><a href="04_diagnostics.html#autocorrelation">Autocorrelation</a></li>
</ul></li>
<li class="has-sub"><a href="04_diagnostics.html#model-checking">Model Checking</a><ul>
<li><a href="04_diagnostics.html#sensitivity-analysis">Sensitivity Analysis</a></li>
<li><a href="04_diagnostics.html#predictive-accuracy-model-comparison">Predictive Accuracy &amp; Model Comparison</a></li>
<li><a href="04_diagnostics.html#posterior-predictive-checking-statistical">Posterior Predictive Checking: Statistical</a></li>
<li><a href="04_diagnostics.html#posterior-predictive-checking-graphical">Posterior Predictive Checking: Graphical</a></li>
</ul></li>
<li><a href="04_diagnostics.html#summary">Summary</a></li>
</ul></li>
<li class="has-sub"><a href="05_enhancements.html#model-enhancements">Model Enhancements</a><ul>
<li><a href="05_enhancements.html#generating-new-variables-of-interest">Generating New Variables of Interest</a></li>
<li><a href="05_enhancements.html#robust-regression">Robust Regression</a></li>
<li><a href="05_enhancements.html#generalized-linear-model">Generalized Linear Model</a></li>
</ul></li>
<li class="has-sub"><a href="06_issues.html#issues">Issues</a><ul>
<li><a href="06_issues.html#debugging">Debugging</a></li>
<li class="has-sub"><a href="06_issues.html#choice-of-prior">Choice of Prior</a><ul>
<li><a href="06_issues.html#noninformative-weakly-informative-informative">Noninformative, Weakly Informative, Informative</a></li>
<li><a href="06_issues.html#conjugacy">Conjugacy</a></li>
<li><a href="06_issues.html#sensitivity-analysis-revisited">Sensitivity Analysis Revisited</a></li>
<li><a href="06_issues.html#summary-1">Summary</a></li>
</ul></li>
<li class="has-sub"><a href="06_issues.html#sampling-procedure">Sampling Procedure</a><ul>
<li><a href="06_issues.html#metropolis">Metropolis</a></li>
<li><a href="06_issues.html#gibbs">Gibbs</a></li>
<li><a href="06_issues.html#hamiltonian-monte-carlo">Hamiltonian Monte Carlo</a></li>
<li><a href="06_issues.html#other-variations-and-approximate-methods">Other Variations and Approximate Methods</a></li>
</ul></li>
<li><a href="06_issues.html#number-of-draws-thinning-warm-up">Number of draws, thinning, warm-up</a></li>
<li><a href="06_issues.html#model-complexity">Model Complexity</a></li>
</ul></li>
<li><a href="1000_Conclusion.html#summary-2">Summary</a></li>
<li class="has-sub"><a href="1001_Appendix.html#appendix">Appendix</a><ul>
<li class="has-sub"><a href="1001_Appendix.html#maximum-likelihood-review">Maximum Likelihood Review</a><ul>
<li><a href="1001_Appendix.html#example">Example</a></li>
</ul></li>
<li><a href="1001_Appendix.html#linear-model">Linear Model</a></li>
<li><a href="1001_Appendix.html#binomial-likelihood-example">Binomial Likelihood Example</a></li>
<li class="has-sub"><a href="1001_Appendix.html#modeling-languages">Modeling Languages</a><ul>
<li><a href="1001_Appendix.html#bugs">Bugs</a></li>
<li><a href="1001_Appendix.html#jags">JAGS</a></li>
<li><a href="1001_Appendix.html#stan">Stan</a></li>
<li><a href="1001_Appendix.html#r">R</a></li>
<li><a href="1001_Appendix.html#general-statistical-packages">General Statistical Packages</a></li>
<li><a href="1001_Appendix.html#other-programming-languages">Other Programming Languages</a></li>
<li><a href="1001_Appendix.html#summary-3">Summary</a></li>
</ul></li>
<li><a href="1001_Appendix.html#bugs-example">BUGS Example</a></li>
<li><a href="1001_Appendix.html#jags-example">JAGS Example</a></li>
<li><a href="1001_Appendix.html#metropolis-hastings-example">Metropolis Hastings Example</a></li>
<li><a href="1001_Appendix.html#hamiltonian-monte-carlo-example">Hamiltonian Monte Carlo Example</a></li>
</ul></li>
<li><a href="1002_references.html#references">References</a></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="model-checking-diagnostics" class="section level1">
<h1>Model Checking &amp; Diagnostics</h1>
<p>As<span class="marginnote">I wonder how many results have been published on models that didnâ€™t converge with the standard MLE. People will often ignore warnings as long as they get a result.</span> with modeling in traditional approaches, it is not enough to simply run a model and get some sort of result. One must examine the results to assess model integrity and have more confidence in the results that have been produced.</p>
<div id="monitoring-convergence" class="section level2">
<h2>Monitoring Convergence</h2>
<p>There are various ways to assess whether or not the model has converged to a target distribution<label for="tufte-sn-19" class="margin-toggle sidenote-number">19</label><input type="checkbox" id="tufte-sn-19" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">19</span> Recall again that we are looking for convergence to a distribution, not a specific parameter.</span>, but as with more complex models in MLE, there is nothing that can tell you for sure that youâ€™ve hit upon <em>the</em> solution. As a starting point, Stan or other modeling environments will spit out repeated warnings or errors if something is egregiously wrong, or perhaps take an extraordinary time to complete relative to expectations, if it ever finishes at all. Assuming youâ€™ve at least gotten past that point, there is more to be done.</p>
<div id="visual-inspection-traceplot-densities" class="section level3">
<h3>Visual Inspection: Traceplot &amp; Densities</h3>
<p>In the previous model we noted the traceplot for a single parameter, and a visual approach to monitoring convergence is certainly one good method. In general we look for a plot that shows random scatter around a mean value, and our model results suggest that the chains mixed well and the traceplot looked satisfactory.<span class="marginnote"><span class="imgbigger"><img src="img/badchains.svg" style="display:block; margin: 0 auto;"  width=75%></span></span></p>
<p>To the right I provide an example where things have gone horribly wrong. The chains never converge nor do they mix with one another. However, one reason for running multiple chains is that any individual chain might converge toward one target, while another chain might converge elsewhere, and this would still be a problem. Also you might see otherwise healthy chains get stuck on occasion over the course of the series, which might suggest more model tweaking or a change in the sampler settings is warranted.</p>
<p>We can examine the mixed chains and density plots of the posterior together with the <span class="pack">rstan</span> or <span class="pack">shinyStan</span> package plot function displayed in the model example or <span class="func">launch_shiny</span> in the <span class="pack">brms</span> package.<span class="marginnote"><span class="imgbigger"><img src="img/shinyStan.png" style="display:block; margin: 0 auto;"></span></span> In the Bayesian approach, increasing amounts of data leads to a posterior distribution of the parameter vector approaching multivariate normality. The figure to the right shows a density, trace and autocorrelation plots for one of the regression coefficients using shinyStan.</p>
</div>
<div id="statistical-measures" class="section level3">
<h3>Statistical Measures</h3>
<p>To go along with visual inspection, we can examine various statistics that might help our determination of convergence or lack thereof. Gelman and Rubinâ€™s <span class="emph">potential scale reduction factor</span>, <span class="math inline">\(\hat{R}\)</span>, provides an estimate of convergence based on the variance of an estimate <span class="math inline">\(\theta\)</span> between chains and the variance within a chain. It is interpreted as the factor by which the variance in the estimate might be reduced with longer chains. We are looking for a value near 1 (and at the very least less than 1.1), and it will get there as <span class="math inline">\(N_{sim} \rightarrow \infty\)</span>.</p>
<p>The <span class="pack">coda</span> package provides other convergence statistics based on Geweke (1992) and Heidelberger and Welch (1983). Along with those statistics, it also has plots for the <span class="math inline">\(\hat{R}\)</span> and Geweke diagnostics.</p>
</div>
<div id="autocorrelation" class="section level3">
<h3>Autocorrelation</h3>
<p>As noted previously, each estimate in the MCMC process is serially correlated with the previous estimates by definition. Furthermore, other aspects of the data, model, and estimation settings may contribute to this. <span class="marginnote"><span class="imgbigger"><img src="img/acfPlotNoSerial.svg" style="display:block; margin: 0 auto;" width=50%></span></span> <span class="marginnote"><span class="imgbigger"><img src="img/acfPlotSerial.svg" style="display:block; margin: 0 auto;" width=50%></span></span> Higher serial correlation typically has the effect of requiring more samples in order to get to a stationary distribution we can feel comfortable with. If inspection of the traceplots look more <em>snake-like</em> than like a fat hairy caterpillar, this might suggest such a situation, and that more samples are required. We can also look at the autocorrelation plot, in which the chainâ€™s correlation with successive lags of the chain are plotted. The first plot to the right is the autocorrelation plot from our model (starting at lag 1). The correlation is low to begin with and then just bounces around zero after. The next plot shows a case of high serial correlation, where the correlation with the first lag is high and the correlation persists even after longer lags. A longer chain with more thinning could help with this.</p>
<p>The effective number of simulation draws is provided as <span class="math inline">\(n_{\textrm{eff}}\)</span> in the Stan output and similarly obtained in BUGS/JAGS. We would desire this number to equal the number of posterior draws requested. In the presence of essentially no autocorrelation the values would be equal. This is not a requirement though, and technically a low number of draws would still be usable. However, a notable discrepancy might suggest there are some issues with the model, or simply that longer chains could be useful.</p>
<p><span class="emph">Monte Carlo error</span> is an estimate of the uncertainty contributed by only having a finite number of posterior draws. Typically weâ€™d want roughly less than 5% of the posterior standard deviation (reported right next to it in the Stan output), but might as well go for less than 1%. With no autocorrelation it would equal <span class="math inline">\(\sqrt{\frac{var(\theta)}{n_{\textrm{eff}}}}\)</span><label for="tufte-sn-20" class="margin-toggle sidenote-number">20</label><input type="checkbox" id="tufte-sn-20" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">20</span> This is the â€˜naiveâ€™ estimate the <span class="pack">coda</span> package provides in its summary output.</span>. and <span class="math inline">\(n_{\textrm{eff}}\)</span> would equal the number of simulation draws requested.</p>
</div>
</div>
<div id="model-checking" class="section level2">
<h2>Model Checking</h2>
<p>Checking the model for suitability is crucial to the analytical process<label for="tufte-sn-21" class="margin-toggle sidenote-number">21</label><input type="checkbox" id="tufte-sn-21" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">21</span> Gelman devotes an entire chapter to this topic to go along with examples of model checking throughout his text. Much of this section follows that outline.</span>. Assuming initial diagnostic inspection for convergence has proven satisfactory, we must then see if the model makes sense in its own right. This can be a straightforward process in many respects, and with Bayesian analysis one has a much richer environment in which to do so compared to traditional methods.</p>
<div id="sensitivity-analysis" class="section level3">
<h3>Sensitivity Analysis</h3>
<p>Sensitivity analysis entails checks on our model settings to see if changes in them, perhaps even slight ones, result in changes in posterior inferences. This may take the form of comparing models with plausible but different priors, different sampling distributions, or differences in other aspects of the model such as the inclusion or exclusion of explanatory variables. While an exhaustive check is impossible, at least some effort in this area should be made.</p>
</div>
<div id="predictive-accuracy-model-comparison" class="section level3">
<h3>Predictive Accuracy &amp; Model Comparison</h3>
<p>A standard way to check for model adequacy is simply to investigate whether the predictions on new data are accurate. In general, the measure of predictive accuracy will be specific to the data problem, and involve posterior simulation of the sort covered in the next section. In addition, while oftentimes new data is hard to come by, assuming one has sufficient data to begin with, one could set aside part of it specifically for this purpose. In this manner one trains and tests the model in much the same fashion as machine learning approaches. In fact, one can incorporate the validation process as an inherent part of the modeling process in the Bayesian context just as you would there.</p>
<p>For model comparison of out of sample predictive performance, there are information measures similar to the Akaike Information criterion (AIC), that one could use in the Bayesian environment. One not to use is the so-called Bayesian information criterion (or BIC), which is not actually Bayesian nor a measure of predictive accuracy. BUGS provides the DIC, or deviance information criterion, as part of its summary output, which is a somewhat Bayesian version of the AIC. More recently developed, the WAIC, or Watanabe-AIC<label for="tufte-sn-22" class="margin-toggle sidenote-number">22</label><input type="checkbox" id="tufte-sn-22" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">22</span> See <a href="http://www.stat.columbia.edu/~gelman/research/published/waic_understand3.pdf">Gelman et al. (2013)</a> for a review and references. See <a href="http://www.stat.columbia.edu/~gelman/research/unpublished/waic_stan.pdf">Vehtari &amp; Gelman (2014)</a> for some more on WAIC, as well as the R package <span class="pack">loo</span>.</span>, is a more fully Bayesian approach. Under some conditions, the DIC and WAIC measures are asymptotically equivalent to Bayesian leave-one-out cross validation, as the AIC is under the classical setting.</p>
</div>
<div id="posterior-predictive-checking-statistical" class="section level3">
<h3>Posterior Predictive Checking: Statistical</h3>
<p>For an overall assessment of model fit, we can examine how well the model can reproduce the data at hand given the <span class="math inline">\(\theta\)</span> draws from the posterior. We discussed earlier the <span class="emph">posterior predictive distribution</span> for a future observation <span class="math inline">\(\tilde{y}\)</span>, <span class="math inline">\(p(\tilde{y}|y) = \int p(\tilde{y}|\theta)p(\theta|y)\mathrm{d}\theta\)</span>, and here weâ€™ll dive in to using it explicitly. There are two sources of uncertainty in our regression model, the variance <span class="math inline">\(\sigma^2\)</span> in y not explained by the model, and posterior uncertainty in the parameters due to having a finite sample size. As <span class="math inline">\(N\rightarrow\infty\)</span>, the latter goes to zero, and so we can simulate draws of <span class="math inline">\(\tilde{y} \sim N(\tilde{X}\beta, \sigma^2I)\)</span><label for="tufte-sn-23" class="margin-toggle sidenote-number">23</label><input type="checkbox" id="tufte-sn-23" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">23</span> Technically, in the conjugate case the posterior predictive is t-distributed because of the uncertainty in the parameters, though it doesnâ€™t take much sample size for simple models to get to approximately normal. Conceptually, with <span class="math inline">\(\hat{\beta}\)</span> being our mean <span class="math inline">\(\beta\)</span> estimate from the posterior, this can be represented as: <br> <span class="math inline">\(\tilde{y} \sim t(\tilde{X}\hat{\beta}, \sigma^2 + \textrm{parUnc}, \textrm{df}=N-k)\)</span></span>. If <span class="math inline">\(\tilde{X}\)</span> is the model data as in the following, then we will refer to <span class="math inline">\(y^{\textrm{Rep}}\)</span> instead of <span class="math inline">\(\tilde{y}\)</span>.</p>
<p>For our model this entails extracting the simulated values from the model object, and taking a random draw from the normal distribution based on the <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\sigma\)</span> that are drawn to produce our <span class="emph">replicated data</span>, <span class="math inline">\(y^{\textrm{Rep}}\)</span> (see <span class="citation">Gelman et al. (<label for="tufte-mn-2" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-2" class="margin-toggle">2013<span class="marginnote">Gelman, Andrew, John B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari, and Donald B. Rubin. 2013. <em>Bayesian Data Analysis</em>. 3rd ed.</span>)</span>, Appendix C).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">betas =<span class="st"> </span>theta$beta
sigmas =<span class="st"> </span>theta$sigma
nsims =<span class="st"> </span><span class="kw">length</span>(theta$sigma)

<span class="co"># produce the replications and inspect</span>
yRep =<span class="st"> </span><span class="kw">sapply</span>(<span class="dv">1</span>:nsims, function(s) <span class="kw">rnorm</span>(<span class="dv">250</span>, X%*%betas[s,], sigmas[s]))
<span class="kw">str</span>(yRep)</code></pre></div>
<pre><code> num [1:250, 1:3000] 4.54 4.22 1.45 3.72 9.06 ...</code></pre>
<p>With the <span class="math inline">\(y^{\textrm{Rep}}\)</span> in hand we can calculate all manner of statistics that might be of interest<label for="tufte-sn-24" class="margin-toggle sidenote-number">24</label><input type="checkbox" id="tufte-sn-24" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">24</span> In many cases you might want to produce this in the generated quantities section of your Stan code, but doing so outside of it will keep the stanfit object smaller, which may be desirable.</span>.</p>
<p>As a starting point, we can check our minimum value among the replicated data sets versus that observed in the data.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">min_rep =<span class="st"> </span><span class="kw">apply</span>(yRep, <span class="dv">2</span>, min)
min_y =<span class="st"> </span><span class="kw">min</span>(y)
<span class="kw">hist</span>(min_rep, <span class="dt">main=</span><span class="st">&#39;&#39;</span>); <span class="kw">abline</span>(<span class="dt">v=</span>min_y)
<span class="kw">c</span>(<span class="kw">mean</span>(min_rep), min_y)</code></pre></div>
<pre><code>[1] -2.828134 -6.056495</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">prop.table</span>(<span class="kw">table</span>(min_rep&gt;min_y))</code></pre></div>
<pre><code>
      FALSE        TRUE 
0.006333333 0.993666667 </code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sort</span>(y)[<span class="dv">1</span>:<span class="dv">5</span>]</code></pre></div>
<pre><code>[1] -6.0564952 -3.2320527 -2.6358579 -2.1649084 -0.8366149</code></pre>
<p><span class="marginnote"><span class="imgbigger"><img src="img/histofyrepMinimum.svg" style="display:block; margin: 0 auto;" width=50%></span></span> These results suggest we may be having difficulty picking up the lower tail of the target variable, as our average minimum is notably higher than that seen in the data, and almost all our minimums are greater than the observed minimum (<span class="math inline">\(p=.99\)</span>). While in this case we know that assuming the normal distribution for our sampling distribution is appropriate, this might otherwise have given us pause for further consideration. A possible solution would be to assume a <span class="math inline">\(t\)</span> distribution for the sampling distribution, which would have fatter tails and thus possibly be better able to handle extreme values. Weâ€™ll show an example of this later. In this case it is just that by chance one of the <span class="math inline">\(y\)</span> values is extreme relative to the others.</p>
<p>In general, we can devise a test statistic, <span class="math inline">\(T_{\textrm{Rep}}\)</span>, and associated p-value to check any particular result of interest based on the simulated data. The p-value in this context is simply the percentage of times the statistic of interest is equal to or more extreme than the statistic, <span class="math inline">\(T_y\)</span>, calculated for the original data. Thus p-values near 0 or 1 are indicative of areas where the model is falling short in some fashion. Sometimes <span class="math inline">\(T_y\)</span> may be based on the <span class="math inline">\(\theta\)</span> parameters being estimated, and thus youâ€™d have a <span class="math inline">\(T_y\)</span> for every posterior draw. In such a case one might examine the scatterplot of <span class="math inline">\(T_{\textrm{Rep}}\)</span> vs. <span class="math inline">\(T_y\)</span>, or examine a density plot of the difference between the two. In short, this is an area where one can get creative. However, it must be stressed that we are not trying to accept or reject a model hypothesis as in the frequentist setting- weâ€™re not going to throw a model out because of an extreme p-value in our posterior predictive check. We are merely trying to understand the modelâ€™s shortcomings as best we can, and make appropriate adjustments if applicable. It is often the case that the model will still be good enough for practical purposes.</p>
</div>
<div id="posterior-predictive-checking-graphical" class="section level3">
<h3>Posterior Predictive Checking: Graphical</h3>
<p><span class="marginnote"><span class="imgbigger"><img src="img/posteriorPredictiveFittedvsObserved.svg" style="display:block; margin: 0 auto;" width=50%></span></span> As there are any number of ways to do statistical posterior predictive checks, we have many options for graphical inspection as well. As a starting point I show a graph of our average fitted value versus the observed data. The average is over all posterior draws of <span class="math inline">\(\theta\)</span>.</p>
<p>Next, I show density plots for a random sample of 20 of the replicated data sets along with that of the original data (shaded). In general it looks like weâ€™re doing pretty well here. The subsequent figure displays the density plot for individual predictions for a single value of <span class="math inline">\(y\)</span> from our data. While it looks like some predictions were low for that value, in general the model captures this particular value of the data decently.<span class="marginnote"><span class="imgbigger"><img src="img/yRepDensity.svg" style="display:block; margin: 0 auto;" width=50%></span></span></p>
<p>We can also examine residual plots of <span class="math inline">\(y - E(y|X,\theta)\)</span> as with standard analysis, shown as the final two figures for this section. The first shows such <em>realized</em> residuals, so-called as they are based on a posterior draw of <span class="math inline">\(\theta\)</span> rather than point estimation of the parameters, versus the expected values from the model.<br />
<span class="marginnote"><span class="imgbigger"><img src="img/posteriorPredictiveFittedY.svg" style="display:block; margin: 0 auto;" width=50%></span></span> The next plot shows the average residual against the average fitted value. No discernible patterns are present, so we may conclude that the model is adequate in this regard. <span class="marginnote"><span class="imgbigger"><img src="img/posteriorPredictiveResidualRaw.svg" style="display:block; margin: 0 auto;" width=50%></span></span> <span class="marginnote"><span class="imgbigger"><img src="img/posteriorPredictiveResidualAvg.svg" style="display:block; margin: 0 auto;" width=50%></span><br>The two plots directly above replicate the figures in 6.11 in Gelman 2013.</span></p>
</div>
</div>
<div id="summary" class="section level2">
<h2>Summary</h2>
<p>As with standard approaches, every model should be checked to see whether it holds up under scrutiny. The previous discussion suggests only a few ways one might go about checking whether the model is worthwhile, but this is a very flexible area where one can answer questions beyond model adequacy and well beyond what traditional models can tell us. Not only is this phase of analysis a necessity, one can use it to explore a vast array of potential questions the data presents, and maybe even answer a few.</p>

</div>
</div>
<p style="text-align: center;">
<a href="03_models.html"><button class="btn btn-default">Previous</button></a>
<a href="05_enhancements.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>



</body>
</html>
